"Fairness is an elusive concept. In the practice of machine learning, the quality of an algorithm is often judged based on its accuracy (the percentage of correct results), its precision (the ability not to label as positive a sample that is negative), or its recall (the ability to find all the positive samples). Deciding which of these three measures is the best proxy for fairness is not always straightforward, and improvements in one metric can cause decreases in others. Moving beyond the machine learning context to frame the discussion in a broader ethical and legal context only makes determining what is truly fair more complicated.
A large piece of the challenge is that MLAs can only be as fair as the data itself. If the underlying data is biased in any way, there is a risk that its structural inequalities will not only be replicated but possibly even amplified in the algorithm. Machine learning engineers must be aware of their own blind spots and implicit assumptions; all the small decisions they make about finding, organizing, and labeling training data for these models can be as impactful as their choice of machine learning techniques. Even more problematic, however, is the issue that societal problems like racial bias, discrimination, and exclusion are deeply rooted in the world around us — and consequently, they are inherent in the data that we extract from the world. For instance, there is evidence that, despite similar rates of drug use, blacks are arrested at four times the rate of whites for drug-related offenses. Even if engineers flawlessly collected this data and trained a perfectly predictive machine learning model with it, the resulting algorithm would still be tainted with the bias embodied in the environment that produced the data.
steps that must be taken to improve algorithmic fairness:
ensure the quality of the data being used to train the algorithms. Retraining periodically with new data to root out historical biases
include a variety of approaches to eradicate bias an standardize them across industry, including unconscious bias training for machine learning engineers similar to the training that intelligence analysts routinely undergo; engineering protocols akin to the protocols of scientific research, such as rigorous peer review; and independent post-implementation auditing of algorithmic fairness which judges the overall quality of an algorithm not only by the standard engineering metrics, but also by how it impacts the most vulnerable people affected by it.
the ownership of these MLAs from the private sector and instead treat them as a public resource that is accessible by all." 
(https://medium.com/berkeleyischool/fairness-in-the-age-of-algorithms-feb11c56a709)

Mila Bachelorarbeit:

Towards Fairness:
The available concepts of fairness can be classified according to different aspects: A common division is the distinction between group and individual fairness, which corresponds to the question ’For whom is fairness achieved?’. Another approach is to subdivide the concepts by their implementation (’when or how?’): while pre-processing the data, as a constraint during training, or while post-processing the outcomes. Since there are several sources of bias, it has also been suggested that more emphasis should be given to the type of bias the different definitions are attempting to mitigate (’what?’).

A naive approach to achieving fairness is to ignore protected attributes like race, sex, disability or cultural membership, “fairness through blindness” or “fairness through unawareness”. Proxies for protected attributes are not easy to predict or detect (Romei and Ruggieri, 2014; Zarsky, 2016), particularly when algorithms access linked datasets (Barocas and Selbst, 2015). Profiles constructed from neutral characteristics such as postal code may inadvertently overlap with other profiles related to ethnicity, gender, sexual preference, and so on (Macnish, 2012; Schermer, 2011). (Copy Paste: Mittelstadt, P.8)
Efforts are underway to avoid such ‘redlining’ by sensitive attributes and proxies. Romei and Ruggieri (2014) observe four overlapping strategies for discrimination prevention in analytics: 
(1) controlled distortion of training data; 
(2) integration of anti-discrimination criteria into the classifier algorithm; 
(3) post-processing of classification models; 
(4) modification of predictions and decisions to maintain a fair proportion of effects between protected and unprotected groups. 
These strategies are seen in the development of privacy preserving, fairness- and discrimination-aware data mining (Dwork et al., 2011; Kamishima et al., 2012). Fairness-aware data mining takes the broadest aim, as it gives attention not only to discrimination but fairness, neutrality, and independence as well (Kamishima et al., 2012). Various metrics of fairness are possible based on statistical parity, differential privacy and other relations between data subjects in classification tasks (Dwork et al., 2011; Romei and Ruggieri, 2014).

Discrimination vs fairness vs bias (Discrimination is the effect of a decision as an adverse disproportionate results impact of algorithmic decision making: barocas &Selbst: disparate impact detection.

Group Fairness:
address systematic disparities (=discrepancies) in outcomes between demographic groups. Within such a group, members are assumed to share certain attributes and especially the sensitive attributes.
Notions of fairness: independence, separation and sufficiency. All three criteria can be formulated purely based on statistical measures as summarized in a confusion matrix. (Barocas and M. Hardt. Fairness in Machine Learning, 2017. URL https://vimeo.com/248490141.)

Independence. 
The notion of independence requires the classifications to be independent of the sensitive attribute. In the case of hiring, independence requires all demographic groups to have equal probability to get hired (or not to get hired). Regarding the task of loan granting, independence requires all groups to be equally likely (or unlikely) to receive a credit loan. It arises from the concept that people should initially have equal chances of being placed in the positive class. Equivalent variants of this notion appear under the terms demographic parity, statistical parity, equal acceptance rates, group fairness or disparate impact.
Reformulations as ratio condition or inequality that allow for error of size e can be viewed as relaxations of independence (“conditional statistical parity). These more relaxed definitions are consistent with legal practice, but may not be sufficient to achieve algorithmic fairness, because the classifier might exploit the leeway (Spielraum, the available freedom to act) to maximize accuracy.
Limitations of independence.
Loss in utility: not allowing the perfect classifier work, because it excludes the correlations between the target labels and sensitive attribute.
Laziness: randomly selection of candidates regardless of their qualifications  Sample size disparity

Separation. 
It requires the classifications to be independent of the sensitive attribute given the true labels. Here, sensitivity (positive prediction given positive true class) and specificity (negative prediction given negative true class) are supposed to be equal for all groups in the population:
loan granting separation requires that all applicants receive a positive (or negative) prediction regardless of group membership, given that they would pay back their loan (default) according to ground truth.

Sufficiency: 
requires the true labels to be independent of the sensitive attribute given the classifier’s predictions.
In the case of loan granting, sufficiency is satisfied when the score already subsumes the sensitive attribute so that the probability to pay back the loan, given a person is granted the loan, i.e. predicted to pay back, and the probability to default, given a negative prediction, is equal across groups. In the context of hiring, people from all groups should have the same probability to be “correctly” hired (or rejected) when they receive a positive (or negative) classification. Here, “correctly” means that the true label matches the predicted one, and that the classifier correctly captures the capabilities of the applicant as measured by the available features.
Similar terms: calibration, conditional use accuracy equality. Similar to separation, sufficiency is optimality compatible, i.e. it allows the classifier to be perfect


Literature:
Kamishima T, Akaho S, Asoh H, et al. (2012) Considerations on fairness-aware data mining. In: IEEE 12th International Conference on Data Mining Workshops, Brussels, Belgium, pp. 378–385. Available at: http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6406465 (accessed 3 November 2015).
Dwork C, Hardt M, Pitassi T, et al. (2011) Fairness through awareness. arXiv:1104.3913 [cs]. Available at: http://arxiv.org/abs/1104.3913 (accessed 15 February 2016).
Barocas S (2014) Data mining and the discourse on discrimination. Available at: https://dataethics.github.io/proceedings/DataMiningandtheDiscourseOnDiscrimination.pdf (accessed 20 December 2015).
Barocas S and Selbst AD (2015) Big data’s disparate impact. SSRN Scholarly Paper, Rochester, NY: Social Science Research Network. Available at: http://papers.ssrn.com/abstract=2477899 (accessed 16 October 2015).
S. Barocas, M. Hardt, and A. Narayanan. Fairness and Machine Learning. fairmlbook.org, 2018. http://www.fairmlbook.org.

S. Barocas and M. Hardt. Fairness in Machine Learning, 2017. URL https://vimeo.com/248490141.
Accessed: 08.01.2019.
Romei A and Ruggieri S (2014) A multidisciplinary survey on discrimination analysis. The Knowledge Engineering Review 29(5): 582–638.
Zarsky T (2016) The trouble with algorithmic decisions an analytic road map to examine efficiency and fairness in automated and opaque decision making. Science, Technology & Human Values 41(1): 118–132.
